{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# People recognition for e-scooters\n",
    "\n",
    "Real time video persons detection and depth prediction.\n",
    " - Mobile Net for objects detection \n",
    " - https://github.com/lindawangg/monodepth2 for depth detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "import matplotlib.pyplot as plt\n",
    "import time \n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import networks\n",
    "from utils import download_model_if_doesnt_exist\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up depth network and loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxim\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and will be removed in 0.15. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\maxim\\AppData\\Roaming\\Python\\Python310\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model_name = \"mono_640x192\"\n",
    "\n",
    "download_model_if_doesnt_exist(model_name)\n",
    "encoder_path = os.path.join(\"models\", model_name, \"encoder.pth\")\n",
    "depth_decoder_path = os.path.join(\"models\", model_name, \"depth.pth\")\n",
    "\n",
    "# LOADING PRETRAINED MODEL\n",
    "encoder = networks.ResnetEncoder(18, False)\n",
    "depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))\n",
    "\n",
    "loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
    "filtered_dict_enc = {k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()}\n",
    "encoder.load_state_dict(filtered_dict_enc)\n",
    "\n",
    "loaded_dict = torch.load(depth_decoder_path, map_location='cpu')\n",
    "depth_decoder.load_state_dict(loaded_dict)\n",
    "\n",
    "encoder.eval()\n",
    "depth_decoder.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth prediction function"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "def depth_prediction(input_image):\n",
    "    im_pil = input_image.convert('RGB')\n",
    "    original_width, original_height = input_image.size\n",
    "\n",
    "    feed_height = loaded_dict_enc['height']\n",
    "    feed_width = loaded_dict_enc['width']\n",
    "    input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        features = encoder(input_image_pytorch)\n",
    "        outputs = depth_decoder(features)\n",
    "\n",
    "    disp = outputs[(\"disp\", 0)]\n",
    "    \n",
    "    disp_resized = torch.nn.functional.interpolate(disp,\n",
    "    (original_height, original_width), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    disp_resized_np = disp_resized.squeeze().cpu().numpy()\n",
    " \n",
    "    im_np = np.asarray(disp_resized_np)\n",
    "    return im_np  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some settings for Mobilenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromCaffe('MobileNetSSD_deploy.prototxt.txt', 'MobileNetSSD_deploy.caffemodel')\n",
    "categories = { 0: 'background', 1: 'aeroplane', 2: 'bicycle', 3: 'bird', 4: 'boat', 5: 'bottle', 6: 'bus', \n",
    "               7: 'car', 8: 'cat', 9: 'chair', 10: 'cow', 11: 'diningtable', 12: 'dog', 13: 'horse', \n",
    "               14: 'motorbike', 15: 'person', 16: 'pottedplant', 17: 'sheep', 18: 'sofa', 19: 'train', 20: 'tvmonitor'}\n",
    "\n",
    "classes =  [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \n",
    "            \"diningtable\",  \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speed transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_to_speed(count):\n",
    "    if count < 1:\n",
    "        speed = 20\n",
    "    elif count >= 1 and count < 3:\n",
    "        speed = 15\n",
    "    elif count >= 3 and count < 5:\n",
    "        speed = 10\n",
    "    elif count >=5:\n",
    "        speed = 5\n",
    "    else:\n",
    "        speed = 25\n",
    "    return speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance determenantion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_wet = [0.46,1.07,1.78,4.33,7.3]  # \n",
    "dist_dry = [0.3,0.88,2.03,3.98,6.13]  # in meters\n",
    "dist_stone = [0.25,0.84,1.87,3.1,5.6] #\n",
    "velocity_interp = [5,10,15,20,25]    # km/h\n",
    "velocity_now = 13\n",
    "th_dist_wet = np.interp(velocity_now, velocity_interp, dist_wet)\n",
    "th_dist_dry = np.interp(velocity_now, velocity_interp, dist_wet)\n",
    "th_dist_stone = np.interp(velocity_now, velocity_interp, dist_wet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objects detection function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image(image):\n",
    "    \n",
    "        im_pil = pil.fromarray(image)\n",
    "        ###########disp_resized_np = depth_prediction(im_pil)\n",
    "    \n",
    "        (h, w) = image.shape[:2]\n",
    "        blob = cv2.dnn.blobFromImage(cv2.resize(image, (300, 300)), 0.007843, (300, 300), 127.5)\n",
    "        net.setInput(blob)\n",
    "        detections = net.forward()\n",
    "        \n",
    "        colors = [[71,40,252]]*len(classes)\n",
    "        count = 0\n",
    "        speed = count_to_speed(count)\n",
    "        for i in np.arange(0, detections.shape[2]):\n",
    "            confidence = detections[0, 0, i, 2]\n",
    "            if confidence > 0.2:\n",
    "                idx = int(detections[0, 0, i, 1])\n",
    "                if idx == 15:\n",
    "                    count += 1\n",
    "                    \n",
    "                    box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "                    (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                    (centerX, centerY) = np.array([(box[0] + box[2]) / 2, (box[1] + box[3]) / 2]).astype(\"int\")\n",
    "                    \n",
    "                    cv2.rectangle(image, (startX, startY), (endX, endY), colors[idx], 2)  \n",
    "                    \n",
    "                    label = \"{}: {:.2f}%\".format(classes[idx], confidence * 100) \n",
    "                    ######depth_label = str(round(disp_resized_np[centerY, centerX], 3))\n",
    "                    speed = count_to_speed(count)\n",
    "                       \n",
    "                    y_label = startY - 15 if startY - 15>15 else startY + 15     \n",
    "                    y_depth = endY - 15\n",
    "                    \n",
    "                    cv2.putText(image, label, (startX, y_label),cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[idx], 2)\n",
    "                    ######cv2.putText(image, \"depth:{}\".format(depth_label), (startX, y_depth),cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[idx], 2)\n",
    "        cv2.putText(image, \"number of people: {}\".format(count), (360, 475-15*2),cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[15], 2)\n",
    "        cv2.putText(image, \"recomended speed, km/h: {}\".format(speed), (360, 475-15),cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[15], 2)\n",
    "        cv2.putText(image, \"current braking distance, m: {}\".format(th_dist_dry), (360, 475),cv2.FONT_HERSHEY_SIMPLEX, 0.5, colors[15], 2)   \n",
    "                    \n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(1)\n",
    "while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image = make_image(image)\n",
    "        cv2.imshow(\"Output\", image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('0'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
